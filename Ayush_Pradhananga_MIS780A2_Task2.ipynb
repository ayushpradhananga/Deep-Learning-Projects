{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushpradhananga/Deep-Learning-Projects/blob/main/Ayush_Pradhananga_MIS780A2_Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZwQc8iIYzhL"
      },
      "source": [
        "\n",
        "\n",
        "Agricultural pest recognition with image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoBihZe6_Acd"
      },
      "source": [
        "## Table of Content\n",
        "\n",
        "1. [Executive Summary](#cell_Summary)\n",
        "\n",
        "2. [Data Preprocessing](#cell_Preprocessing)\n",
        "\n",
        "3. [Predictive Modeling](#cell_model)\n",
        "\n",
        "4. [Experiments Report](#cell_report)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id = \"cell_Summary\"></a>\n",
        "## 1. Executive Summary"
      ],
      "metadata": {
        "id": "rzo8kho_wzib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Definition:\n",
        "In this project the problem is classification of agricultural pests in images. Specifically, we have a dataset comprising 2,479 real images of five different types of agricultural pests: Ants, Bees, Grasshoppers, Moths, and Wasps. The goal is to build and evaluate machine learning models capable of accurately detecting and classifying these pests within images.\n",
        "\n",
        "\n",
        "Proposed Approaches:\n",
        "Two deep learning models with different architectures will be developed and compared for pest recognition. These models will involve various combinations of convolutional layers, pooling layers, fully connected layers, and dropout layers.\n",
        "\n",
        "\n",
        "Major Findings:\n",
        "\n",
        "Both the model was not reliable and performed poorly on the testing data. However, model 1 gave a better kappa. The models require alot of improvements such as hyper parameter tuning and improvement in model architechture. This is the limitation of this project.\n",
        "\n",
        "Real-world Applicability: A robust model would have been applicable for agricultre sector. However, the models presented is not reliable for any business decision."
      ],
      "metadata": {
        "id": "NRFjf1aO5le5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqPl5gh_YzhX"
      },
      "source": [
        "<a id = \"cell_Preprocessing\"></a>\n",
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<style>table {float:left}</style>\n",
        "<style>img {float:left}</style>"
      ],
      "metadata": {
        "id": "XEIvx87g2M-W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "25e382ab-39d4-4815-aceb-4e4c0db27623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table {float:left}</style>\n",
              "<style>img {float:left}</style>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Import Libraries"
      ],
      "metadata": {
        "id": "4Zawpu4irhgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np                   # Import NumPy for numerical operations.\n",
        "import matplotlib.pyplot as plt      # Import Matplotlib for data visualization.\n",
        "import random                        # Import random for randomization (used for various purposes).\n",
        "from tensorflow import keras         # Import TensorFlow's Keras API.\n",
        "import tensorflow as tf              # Import TensorFlow for low-level deep learning functionality.\n",
        "import os                            # Import the 'os' module for operating system-related functions and utilities.\n",
        "\n",
        "from sklearn.metrics import classification_report   # Import classification_report for generating classification performance metrics.\n",
        "from sklearn.metrics import cohen_kappa_score       # Import cohen_kappa_score for calculating the Cohen's Kappa coefficient.\n",
        "from sklearn.metrics import confusion_matrix        # Import confusion_matrix for creating confusion matrices.\n",
        "from sklearn.metrics import ConfusionMatrixDisplay   # Import ConfusionMatrixDisplay for displaying confusion matrices.\n",
        "\n",
        "from keras.datasets import cifar10       # Import the CIFAR-10 dataset, a popular image classification dataset.\n",
        "from keras.models import Sequential    # Import Sequential for creating a sequential deep learning model.\n",
        "from keras.layers.core import Dense, Dropout, Activation  # Import core components for building neural network layers.\n",
        "from keras.utils import np_utils     # Import np_utils for handling data transformations.\n",
        "from keras.preprocessing.image import ImageDataGenerator  # Import ImageDataGenerator for data augmentation.\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten\n",
        "# Import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, and Flatten for defining convolutional and pooling layers.\n",
        "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
        "# Import BatchNormalization for normalizing layer activations.\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping  # Import ModelCheckpoint and EarlyStopping for model callbacks.\n",
        "from keras.optimizers import SGD, RMSprop, Adam, Nadam  # Import various optimizers for training the model.\n",
        "from keras.losses import categorical_crossentropy  # Import categorical_crossentropy as the loss function for multi-class classification.\n",
        "\n",
        "# This section sets up your deep learning environment and imports the necessary libraries and modules.\n"
      ],
      "metadata": {
        "id": "7fD91JE22aZm",
        "outputId": "d679c27f-e3f6-4f70-8f4c-5b8edcf6bfc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-391e1be3f3cc>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcifar10\u001b[0m       \u001b[0;31m# Import the CIFAR-10 dataset, a popular image classification dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m    \u001b[0;31m# Import Sequential for creating a sequential deep learning model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m  \u001b[0;31m# Import core components for building neural network layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m     \u001b[0;31m# Import np_utils for handling data transformations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m  \u001b[0;31m# Import ImageDataGenerator for data augmentation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.core'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check GPUs on this machine\n",
        "tf.config.list_physical_devices('GPU')\n"
      ],
      "metadata": {
        "id": "sFRotySX2cuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to plot image"
      ],
      "metadata": {
        "id": "g-IVbldCpkFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#A custom function for plotting a grid of images\n",
        "def plot_images(ims, figsize=(12,12), cols=1, interp=False, titles=None):\n",
        "    if type(ims[0]) is np.ndarray:\n",
        "        if (ims.shape[-1] != 3):\n",
        "            ims = ims = ims[:,:,:,0]\n",
        "    f = plt.figure(figsize=figsize)\n",
        "    rows=len(ims)//cols if len(ims) % cols == 0 else len(ims)//cols + 1\n",
        "    for i in range(len(ims)):\n",
        "        sp = f.add_subplot(rows, cols, i+1)\n",
        "        sp.axis('Off')\n",
        "        if titles is not None:\n",
        "            sp.set_title(titles[i], fontsize=16)\n",
        "        plt.imshow(ims[i], interpolation=None if interp else 'none')"
      ],
      "metadata": {
        "id": "4MRHa1zz2fxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to plot history"
      ],
      "metadata": {
        "id": "7QiiekxZpoc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_hist(h, xsize=6, ysize=5):\n",
        "    # Prepare plotting\n",
        "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "    plt.rcParams[\"figure.figsize\"] = [xsize, ysize]\n",
        "\n",
        "    # Get training and validation keys\n",
        "    ks = list(h.keys())\n",
        "    n2 = math.floor(len(ks)/2)\n",
        "    train_keys = ks[0:n2]\n",
        "    valid_keys = ks[n2:2*n2]\n",
        "\n",
        "    # summarize history for different metrics\n",
        "    for i in range(n2):\n",
        "        plt.plot(h[train_keys[i]])\n",
        "        plt.plot(h[valid_keys[i]])\n",
        "        plt.title('Training vs Validation '+train_keys[i])\n",
        "        plt.ylabel(train_keys[i])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.draw()\n",
        "        plt.show()\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "faGWfHvd2jiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mounting Google Drive"
      ],
      "metadata": {
        "id": "bxIBWQkmpvFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for access.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List files in a specific directory.\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/Part2_agricultural_pests/\""
      ],
      "metadata": {
        "id": "nfylIErt4K3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import data"
      ],
      "metadata": {
        "id": "LaBNSYcGz4Ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the paths to the folders containing the image files\n",
        "ants_path = '/content/drive/MyDrive/Colab Notebooks/Part2_agricultural_pests/ants'\n",
        "bees_path = '/content/drive/MyDrive/Colab Notebooks/Part2_agricultural_pests/bees'\n",
        "grasshopper_path = '/content/drive/MyDrive/Colab Notebooks/Part2_agricultural_pests/grasshopper'\n",
        "moth_path = '/content/drive/MyDrive/Colab Notebooks/Part2_agricultural_pests/moth'\n",
        "wasp_path = '/content/drive/MyDrive/Colab Notebooks/Part2_agricultural_pests/wasp'\n",
        "# get a list of all files in the folder\n",
        "ants_file_list = os.listdir(ants_path)\n",
        "bees_file_list = os.listdir(bees_path)\n",
        "grasshopper_file_list = os.listdir(grasshopper_path)\n",
        "moth_file_list = os.listdir(moth_path)\n",
        "wasp_file_list = os.listdir(wasp_path)\n",
        "# print the total number of files\n",
        "print(f'Total number of files under ants folder are: {len(ants_file_list)}')\n",
        "print(f'Total number of files under bees folder are: {len(bees_file_list)}')\n",
        "print(f'Total number of files under grasshopper folder are: {len(grasshopper_file_list)}')\n",
        "print(f'Total number of files under moth folder are: {len(moth_file_list)}')\n",
        "print(f'Total number of files under wasp folder are: {len(wasp_file_list)}')"
      ],
      "metadata": {
        "id": "xHiWkq-uKly7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read and decode raw image data from two folders then Assign labels based on the folder After that we Resize images to 100x100 resolution. Lastly, Store image data and labels in a list called 'data'."
      ],
      "metadata": {
        "id": "2Kv9yxXvsFVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to store the image data and labels\n",
        "data = []\n",
        "\n",
        "# Iterate through the files in the first folder\n",
        "for file in os.listdir(ants_path):\n",
        "  # Check if the file is a jpeg or jpg file\n",
        "  if file.endswith('.jpeg') or file.endswith('.jpg'):\n",
        "    # Load the image data from the file using TensorFlow\n",
        "    img = tf.io.read_file(os.path.join(ants_path, file))\n",
        "    img = tf.image.decode_jpeg(img)\n",
        "    img = tf.image.resize(img, (100, 100))\n",
        "    # Assign a label to the file\n",
        "    label = 'Ants'\n",
        "    # Add the image data and label to the data list\n",
        "    data.append((img, label))\n",
        "\n",
        "# Iterate through the files in the second folder\n",
        "for file in os.listdir(bees_path):\n",
        "  # Check if the file is a jpeg or jpg file\n",
        "  if file.endswith('.jpeg') or file.endswith('.jpg'):\n",
        "    # Load the image data from the file using TensorFlow\n",
        "    img = tf.io.read_file(os.path.join(bees_path, file))\n",
        "    img = tf.image.decode_jpeg(img)\n",
        "    img = tf.image.resize(img, (100, 100))\n",
        "    # Assign a label to the file\n",
        "    label = 'Bees'\n",
        "    # Add the image data and label to the data list\n",
        "    data.append((img, label))\n",
        "\n",
        "# Iterate through the files in the third folder\n",
        "for file in os.listdir(grasshopper_path):\n",
        "  # Check if the file is a jpeg or jpg file\n",
        "  if file.endswith('.jpeg') or file.endswith('.jpg'):\n",
        "    # Load the image data from the file using TensorFlow\n",
        "    img = tf.io.read_file(os.path.join(grasshopper_path, file))\n",
        "    img = tf.image.decode_jpeg(img)\n",
        "    img = tf.image.resize(img, (100, 100))\n",
        "    # Assign a label to the file\n",
        "    label = 'Grasshopper'\n",
        "    # Add the image data and label to the data list\n",
        "    data.append((img, label))\n",
        "\n",
        "# Iterate through the files in the fourth folder\n",
        "for file in os.listdir(moth_path):\n",
        "  # Check if the file is a jpeg or jpg file\n",
        "  if file.endswith('.jpeg') or file.endswith('.jpg'):\n",
        "    # Load the image data from the file using TensorFlow\n",
        "    img = tf.io.read_file(os.path.join(moth_path, file))\n",
        "    img = tf.image.decode_jpeg(img)\n",
        "    img = tf.image.resize(img, (100, 100))\n",
        "    # Assign a label to the file\n",
        "    label = 'Moth'\n",
        "    # Add the image data and label to the data list\n",
        "    data.append((img, label))\n",
        "\n",
        " # Iterate through the files in the fifth folder\n",
        "for file in os.listdir(wasp_path):\n",
        "  # Check if the file is a jpeg or jpg file\n",
        "  if file.endswith('.jpeg') or file.endswith('.jpg'):\n",
        "    # Load the image data from the file using TensorFlow\n",
        "    img = tf.io.read_file(os.path.join(wasp_path, file))\n",
        "    img = tf.image.decode_jpeg(img)\n",
        "    img = tf.image.resize(img, (100, 100))\n",
        "    # Assign a label to the file\n",
        "    label = 'Wasp'\n",
        "    # Add the image data and label to the data list\n",
        "    data.append((img, label))"
      ],
      "metadata": {
        "id": "BPqNKfqvMt0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train- Test Split"
      ],
      "metadata": {
        "id": "p8jf4hfButXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input shape based on image dimensions (100x100 pixels with 3 color channels).\n",
        "input_shape = (100, 100, 3)\n",
        "\n",
        "# Specify the number of classes:  5 classes).\n",
        "num_classes = 5\n",
        "\n",
        "# Split the shuffled data into training and testing sets (70% train, 30% test).\n",
        "train_data, test_data = data[:int(len(data) * 0.7)], data[int(len(data) * 0.7):]\n"
      ],
      "metadata": {
        "id": "O_5FdooOOJY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training data consits of 70% and testing data consits of 30%.\n",
        "Allocate X_train, X_test, Y_train, and Y_test.\n",
        "Convert them into NumPy arrays for use in training a Convolutional Neural Network (CNN) module.\n"
      ],
      "metadata": {
        "id": "aZpymK3Vt9Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the image data and labels from the training data\n",
        "X_train, Y_train = zip(*train_data)\n",
        "\n",
        "# Extract the image data and labels from the testing data\n",
        "X_test, Y_test = zip(*test_data)\n",
        "\n",
        "# Convert the image data and labels into NumPy arrays\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "X_test = np.array(X_test)\n",
        "Y_test = np.array(Y_test)\n",
        "\n",
        "# Reshape the training data\n",
        "X_train = X_train.reshape(X_train.shape[0], *input_shape)\n",
        "\n",
        "# Reshape the test data\n",
        "X_test = X_test.reshape(X_test.shape[0], *input_shape)"
      ],
      "metadata": {
        "id": "cQ-z49yuOaps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Normalization"
      ],
      "metadata": {
        "id": "Xtrf3Jcgu159"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "# change integers to 32-bit floating point numbers\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# normalize each value for each pixel for the entire vector for each input\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# print the shape of the reshaped data\n",
        "print(\"Training matrix shape\", X_train.shape)\n",
        "print(\"Testing matrix shape\", X_test.shape)"
      ],
      "metadata": {
        "id": "P2s98KjJOd9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('The original format of class of the first element in the training dataset is: ',Y_train[0], '\\n')\n",
        "\n",
        "import numpy as np\n",
        "# Create a NumPy array with category strings\n",
        "categories = np.array(['Ants', 'Bees', 'Grasshopper', 'Moth', 'Wasp'])\n",
        "\n",
        "# Create a mapping from category strings to integers\n",
        "category_map = {'Ants': 0, 'Bees': 1, 'Grasshopper': 2, 'Moth': 3, 'Wasp': 4}\n",
        "\n",
        "# Encode the categories for both training and test sets\n",
        "Y_train = np.array([category_map[category] for category in Y_train])\n",
        "Y_test = np.array([category_map[category] for category in Y_test])\n",
        "\n",
        "print('The unique integer mapping encoding format of the class of the first element in the training dataset is: ',Y_train[0])\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = to_categorical(Y_train, num_classes)\n",
        "Y_test = to_categorical(Y_test, num_classes)\n",
        "# print the shape of the reshaped data\n",
        "print(\"Training matrix shape\", Y_train.shape)\n",
        "print(\"Testing matrix shape\", Y_test.shape)"
      ],
      "metadata": {
        "id": "_LGlVzslOiV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting image from the training data"
      ],
      "metadata": {
        "id": "fARg-QbZvNCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the default figure size\n",
        "plt.rcParams['figure.figsize'] = (9, 9)\n",
        "\n",
        "labels = ['Ants', 'Bees', 'Grasshopper', 'Moth', 'Wasp']\n",
        "\n",
        "# Convert Y_train to integer labels if it's one-hot encoded\n",
        "integer_labels = np.argmax(Y_train, axis=1)\n",
        "\n",
        "# Create a 5x5 grid of subplots\n",
        "fig, axes = plt.subplots(5, 5, figsize=(15, 15))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    # Display the image in the current subplot\n",
        "    ax.imshow(X_train[i], cmap='gray')\n",
        "    ax.set_title(\"{}\".format(labels[integer_labels[i]]))\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zwa9TDfEPyuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY3RPGcKYzhc"
      },
      "source": [
        "<a id = \"cell_model\"></a>\n",
        "## 3. Predictive Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First Model"
      ],
      "metadata": {
        "id": "iayeTPLixZrI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRSO49oTYzhc"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def model_1(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(4, 4), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EarlyStopping callback is used from Keras to stop the model training if the validation loss stops decreasing for a few epochs."
      ],
      "metadata": {
        "id": "TM5gQddPwCSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras callbacks (when Tensorboard installed)\n",
        "keras_callbacks = [EarlyStopping(monitor='val_loss', patience=10, verbose=0)]"
      ],
      "metadata": {
        "id": "XVaWaYbfQZTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_1(input_shape, num_classes)"
      ],
      "metadata": {
        "id": "lk3nk9AbQc1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've designed my model with a specific architecture in mind. It starts with a Convolutional Layer (Conv2D) with 32 filters and a 4x4 kernel size. This layer helps the model learn important features from the input images using the Rectified Linear Unit (ReLU) activation function.\n",
        "\n",
        "After this Conv2D layer, I've added a MaxPooling Layer (MaxPooling2D) with a 2x2 pooling size. This layer reduces the spatial dimensions and retains crucial information from the previous layer.\n",
        "\n",
        "Following the MaxPooling layer, I've incorporated a Flatten Layer. This step is crucial as it transforms the 2D feature maps into a 1D vector, making it suitable for further processing in the dense layers.\n",
        "\n",
        "The Fully Connected Layers (Dense) are next in line. The first Dense layer consists of 50 neurons, employing the ReLU activation function. Right after this layer, I've included a Dropout layer with a dropout rate of 0.25. Dropout helps prevent overfitting by randomly deactivating neurons during training.\n",
        "\n",
        "Lastly, there's the final Dense layer, which has a number of neurons equal to num_classes. This layer utilizes the softmax activation function, which is particularly useful for multi-class classification tasks. It provides class probabilities as its output, enabling the model to make predictions based on these probabilities."
      ],
      "metadata": {
        "id": "axMLgZO0wOKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with the Adam optimizer\n",
        "model.compile(loss=categorical_crossentropy,\n",
        "              optimizer=Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n",
        "              metrics='accuracy')"
      ],
      "metadata": {
        "id": "5qiIiqjcSZJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(X_train, Y_train,\n",
        "      batch_size=128,\n",
        "      epochs=50,\n",
        "      verbose=2,\n",
        "      validation_data=(X_test, Y_test),\n",
        "      validation_split=0.3,\n",
        "      callbacks=keras_callbacks)"
      ],
      "metadata": {
        "id": "rvGVq3p7SDMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on training data\n",
        "train_score = model.evaluate(X_train, Y_train, verbose=0)\n",
        "print('Train loss:', (train_score[0], 4))\n",
        "print('Train accuracy:', (train_score[1], 4), '\\n')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test loss:', (test_score[0], 4))\n",
        "print('Test accuracy:', (test_score[1], 4))"
      ],
      "metadata": {
        "id": "RvpIyieEhSPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "# Convert y_pred to categorical labels\n",
        "predicted_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert Y_test to categorical labels\n",
        "true_labels = np.argmax(Y_test, axis=1)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Calculate the kappa score\n",
        "kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
        "print(\"The result of Kappa is:\", round(kappa, 3))\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=labels)\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "OhhbUzwkCN9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "plot_hist(pd.DataFrame(hist.history))"
      ],
      "metadata": {
        "id": "0EXLj9YGhZz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Second Model"
      ],
      "metadata": {
        "id": "-iY8OBUjCq3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import BatchNormalization, Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2  # Import L2 regularization\n",
        "\n",
        "def model_2():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(10, kernel_size=(3, 3),\n",
        "                     activation='relu',\n",
        "                     input_shape=input_shape))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "6nqIruU8CtTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complexmodel = model_2()"
      ],
      "metadata": {
        "id": "eU7aRPBwDTRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with the RMSprop optimizer\n",
        "complexmodel.compile(loss=categorical_crossentropy,\n",
        "              optimizer= RMSprop(lr=0.001,decay=1e-6),\n",
        "              metrics='accuracy')"
      ],
      "metadata": {
        "id": "whNZNXVwEvYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = complexmodel.fit(X_train, Y_train,\n",
        "      batch_size=128,\n",
        "      epochs=100,\n",
        "      verbose=2,\n",
        "      validation_data=(X_test, Y_test),\n",
        "      validation_split=0.3,\n",
        "      callbacks=keras_callbacks)"
      ],
      "metadata": {
        "id": "w6j9OSNFFUUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on training data\n",
        "train_score1 = complexmodel.evaluate(X_train, Y_train, verbose=0)\n",
        "print('Train loss:', (train_score1[0], 4))\n",
        "print('Train accuracy:', (train_score1[1], 4), '\\n')\n",
        "\n",
        "# Evaluate on test data\n",
        "test_score1 = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test loss:', (test_score1[0], 4))\n",
        "print('Test accuracy:', (test_score1[1], 4))"
      ],
      "metadata": {
        "id": "lNkxB7ptFeXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred1 = complexmodel.predict(X_test)\n",
        "# Convert y_pred to categorical labels\n",
        "predicted_labels1 = np.argmax(y_pred1, axis=1)\n",
        "\n",
        "# Convert Y_test to categorical labels\n",
        "true_labels1 = np.argmax(Y_test, axis=1)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Calculate the kappa score\n",
        "kappa1 = cohen_kappa_score(true_labels1, predicted_labels1)\n",
        "print(\"The result of Kappa is:\", round(kappa1, 3))"
      ],
      "metadata": {
        "id": "aS3riu7zKYke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_hist(pd.DataFrame(history.history))"
      ],
      "metadata": {
        "id": "HrgZBNdLHTrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLBXWEhpYzhg"
      },
      "source": [
        "<a id = \"cell_report\"></a>\n",
        "## 4. Experiments Report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first model performed better than the second model, although both the models are not good. The first model got an accuracy of 21.8% and kappa of 15.1% on testing data. The second model on the other hand got the same accuracy but a lower kappa of 13%."
      ],
      "metadata": {
        "id": "puR7WPh9Hixp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The models could be improved with changing the architecture and hyperparameters. However, upon experimenting, the more complex the model I built, the worse was the performance. The model is not reliable and can not be used for a real business decision. The model needs to be improved before using it for any business purpose."
      ],
      "metadata": {
        "id": "Odghn_oZIh3O"
      }
    }
  ]
}